{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/01 22:23:44 WARN Utils: Your hostname, dj resolves to a loopback address: 127.0.1.1; using 192.168.0.111 instead (on interface wlp0s20f3)\n",
      "23/11/01 22:23:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/deepak/Downloads/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/deepak/.ivy2/cache\n",
      "The jars for the packages stored in: /home/deepak/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cd3c0e2a-943d-4277-8537-2ea96489ef01;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 539ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cd3c0e2a-943d-4277-8537-2ea96489ef01\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/19ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/01 22:23:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trying_delta\")\\\n",
    ".config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0\")\\\n",
    ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    ".config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip='src/input.csv'\n",
    "op='op/'\n",
    "nK=[\"id\"]\n",
    "hashCol=\"SHA256\"\n",
    "sKey=\"sK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|name| age|\n",
      "+---+----+----+\n",
      "|  1|   X|  10|\n",
      "|  2|   B|  11|\n",
      "|  3|   C|null|\n",
      "|  4|null|   5|\n",
      "|  5|   D|   6|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "        .read\n",
    "        .format('csv')\n",
    "        .option('header', 'true')\n",
    "        .option('inferSchema', 'true')\n",
    "        .load(ip))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleIntegerType(df:DataFrame,cols:list[str])->DataFrame:\n",
    "    return df.na.fill(0,cols)\n",
    "\n",
    "def handleStringType(df:DataFrame,cols:list[str])->DataFrame:\n",
    "    df=df.na.fill(\"NA\",cols)\n",
    "    for c in cols:\n",
    "        df=df.withColumn(c,trim(df[c]))\n",
    "    return df\n",
    "        \n",
    "def cleanDF(df:DataFrame)->DataFrame:\n",
    "    d={}\n",
    "    for field in df.schema:\n",
    "        fieldType=str(field.dataType).replace(\"()\",\"\")\n",
    "        if((fieldType in d.keys()) == False):\n",
    "            d[fieldType]=[field.name]\n",
    "        else:\n",
    "            d[fieldType].append(field.name)\n",
    "\n",
    "    for key,cols  in d.items():\n",
    "        if key=='IntegerType': df=df.transform(handleIntegerType,cols);\n",
    "        elif key=='StringType': df=df.transform(handleStringType,cols);\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1|   X| 10|\n",
      "|  2|   B| 11|\n",
      "|  3|   C|  0|\n",
      "|  4|  NA|  5|\n",
      "|  5|   D|  6|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.transform(cleanDF)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addHash(df:DataFrame,nK:list[str])->DataFrame:\n",
    "    df= df.withColumn(hashCol,sha2(concat_ws(\"_\",*nK),256))\n",
    "    return df\n",
    "\n",
    "def addSkey(df:DataFrame)->DataFrame:\n",
    "    return df.withColumn(sKey,row_number().over(Window.orderBy(hashCol)))\n",
    "\n",
    "def getDeltaDF():\n",
    "    if os.path.exists(op):\n",
    "        deltaDF=DeltaTable.forPath(spark,op)\n",
    "        return deltaDF\n",
    "    return 0\n",
    "\n",
    "def fullLoad(df:DataFrame,nK:list[str]):\n",
    "   df=df.transform(addHash,nK)\n",
    "   return df.transform(addSkey) \n",
    "\n",
    "def deltaLoad(srdDF:DataFrame,deltaDF:DeltaTable,nK:list[str]): \n",
    "    tempDeltaDF=deltaDF.toDF().select(hashCol,sKey)\n",
    "    maxKey=tempDeltaDF.select(max(sKey)).first()[f\"max({sKey})\"]\n",
    "\n",
    "    srcDF=srdDF.transform(addHash,nK)\n",
    "    fullDF=srcDF.join(tempDeltaDF,[hashCol],\"full\")\n",
    "    newRecordsDF=fullDF.filter(\"sk is null\").transform(addSkey).withColumn(sKey,maxKey+col(sKey)+1)\n",
    "    fullDF=fullDF.filter(\"sk is not null\").union(newRecordsDF)\n",
    "    # fullDF.show()\n",
    "    return fullDF\n",
    "    \n",
    "    # if(newRecordsDF.count()==0):print(f\" No records with new naturalKeys {nK}, Checking for Updated Records  \")\n",
    "    # else:\n",
    "    #     newRecordsDF=newRecordsDF.transform(addSkey)\n",
    "    #     maxKey=deltaDF.toDF().select(max(sKey)).first()[f\"max({sKey})\"]\n",
    "    #     newRecordsDF=newRecordsDF.withColumn(sKey,df.sKey+maxKey+1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/01 22:46:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:46:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:46:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:46:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:46:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:46:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:46:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+--------------------+---+----+---+---+\n",
      "|              SHA256| id|name|age| sK|\n",
      "+--------------------+---+----+---+---+\n",
      "|4b227777d4dd1fc61...|  4|  NA|  5|  1|\n",
      "|4e07408562bedb8b6...|  3|   C|  0|  2|\n",
      "|6b86b273ff34fce19...|  1|   X| 10|  3|\n",
      "|d4735e3a265e16eee...|  2|   B| 11|  4|\n",
      "|ef2d127de37b942ba...|  5|   D|  6|  6|\n",
      "+--------------------+---+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaDF=getDeltaDF()\n",
    "\n",
    "if deltaDF==0:\n",
    "    resDF=fullLoad(df,nK)\n",
    "    resDF.write.format(\"delta\").save(op)\n",
    "else:\n",
    "    resDF=deltaLoad(df,deltaDF,nK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/01 22:45:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:45:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:45:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:45:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:45:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:45:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/01 22:45:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+--------------------+---+----+---+----+\n",
      "|              SHA256| id|name|age|  sK|\n",
      "+--------------------+---+----+---+----+\n",
      "|ef2d127de37b942ba...|  5|   D|  6|null|\n",
      "|ef2d127de37b942ba...|  5|   D|  6|   6|\n",
      "+--------------------+---+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaLoad(df,deltaDF,nK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
